# v0.6.0 Implementation Plan - VibeVoice-ASR Core + Auto-Processing

Last updated: 2026-02-13

## Overview

v0.6.0 introduces **VibeVoice-ASR integration** with comprehensive auto-processing capabilities for meeting recordings. This release builds on v0.5.0's long-form features to provide speaker diarization, automated analysis, and intelligent meeting documentation.

## Timeline

**Estimated Duration**: 6-8 weeks

**Effort Breakdown**:

- Core VibeVoice integration: 2 weeks
- OPUS recording/FFmpeg integration: 1 week
- Auto-processing pipeline: 2 weeks
- Settings UI + model updates: 1 week
- Testing + polish: 1-2 weeks

## Goals

1. Enable speaker-diarized transcription for meeting recordings
2. Provide configurable quality settings (FP16/INT8) for different hardware
3. Implement OPUS-based recording for efficient storage
4. Build auto-processing pipeline for meeting analysis
5. Generate meeting artifacts (minutes, summaries, chapters)
6. Support model updates with user-friendly notifications

## Hardware Context

**Target Hardware**: RTX 5070 Ti with 16GB VRAM

**Supported Configurations**:

- **FP16 Mode**: Full precision, ~14-16 GB VRAM, highest quality
- **INT8 Mode**: Quantized, ~7-8 GB VRAM, slight quality trade-off
- **Sequential Processing** (default): Whisper ‚Üí VibeVoice-ASR separately
- **Parallel Processing** (optional): Both models simultaneously (INT8 recommended)

## Architecture

### System Components

```
Trispr Flow App
     ‚îÇ
     ‚îú‚îÄ‚îÄ Output Capture
     ‚îÇ   ‚îî‚îÄ‚îÄ FFmpeg Encoder ‚îÄ‚îÄ‚ñ∫ meeting_YYYY-MM-DD_HH-MM.opus
     ‚îÇ
     ‚îú‚îÄ‚îÄ Analysis Trigger (Manual or Auto)
     ‚îÇ   ‚îî‚îÄ‚îÄ POST localhost:8321/transcribe
     ‚îÇ       { audio_path, precision: "fp16"|"int8", parallel: bool }
     ‚îÇ
     ‚îú‚îÄ‚îÄ VibeVoice-ASR Sidecar (Python FastAPI)
     ‚îÇ   ‚îú‚îÄ‚îÄ Model Loading (Lazy, FP16/INT8)
     ‚îÇ   ‚îú‚îÄ‚îÄ FFmpeg Decoder (OPUS ‚Üí PCM)
     ‚îÇ   ‚îú‚îÄ‚îÄ Speaker Diarization
     ‚îÇ   ‚îî‚îÄ‚îÄ JSON Response: { segments: [{speaker_id, start, end, text}] }
     ‚îÇ
     ‚îú‚îÄ‚îÄ Auto-Processing Pipeline
     ‚îÇ   ‚îú‚îÄ‚îÄ Chapter Generator (topic shifts + silence detection)
     ‚îÇ   ‚îú‚îÄ‚îÄ Meeting Minutes Generator (AI-powered)
     ‚îÇ   ‚îî‚îÄ‚îÄ Summary/Analysis Generator
     ‚îÇ
     ‚îú‚îÄ‚îÄ Export Engine
     ‚îÇ   ‚îú‚îÄ‚îÄ Speaker-diarized transcript (JSON + Visual HTML)
     ‚îÇ   ‚îú‚îÄ‚îÄ Video with karaoke overlay (FFmpeg)
     ‚îÇ   ‚îú‚îÄ‚îÄ Meeting Minutes (MD/PDF via Pandoc)
     ‚îÇ   ‚îî‚îÄ‚îÄ Chapter-indexed transcript
     ‚îÇ
     ‚îî‚îÄ‚îÄ Model Update Monitor
         ‚îú‚îÄ‚îÄ Version checker (HuggingFace API)
         ‚îú‚îÄ‚îÄ Release notes parser
         ‚îî‚îÄ‚îÄ User notification UI
```

### Sidecar Architecture

```
sidecar/vibevoice-asr/
     ‚îú‚îÄ‚îÄ server.py              # FastAPI application
     ‚îú‚îÄ‚îÄ model_loader.py        # FP16/INT8 model loading
     ‚îú‚îÄ‚îÄ audio_processor.py     # FFmpeg integration
     ‚îú‚îÄ‚îÄ diarizer.py            # Speaker diarization logic
     ‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
     ‚îú‚îÄ‚îÄ pyproject.toml         # Project metadata
     ‚îî‚îÄ‚îÄ build.py               # PyInstaller build script
```

## Features

### 1. VibeVoice-ASR Core Integration

#### 1.1 Model Loading

**Goal**: Load Microsoft VibeVoice-ASR 7B model with configurable precision

**Capabilities**:

- Up to 60 minutes continuous audio in a single pass
- Built-in speaker diarization (who, when, what)
- Timestamps per segment
- 50+ languages
- Customizable hotwords for domain-specific terms

**Implementation**:

```python
# model_loader.py
class VibeVoiceLoader:
    def __init__(self, precision: str = "fp16"):
        self.precision = precision
        self.model = None

    def load_model(self):
        """Lazy load model into VRAM"""
        if self.model is None:
            if self.precision == "fp16":
                self.model = load_fp16_model()
            else:
                self.model = load_int8_model()
        return self.model

    def unload_model(self):
        """Free VRAM"""
        del self.model
        self.model = None
        torch.cuda.empty_cache()
```

**Settings**:

- Precision mode: FP16 / INT8 (dropdown)
- Auto-load on startup: Yes / No (checkbox)
- Model cache directory: Path selector

**Tasks**:

- [ ] Implement model loader with FP16/INT8 support
- [ ] Add lazy loading mechanism
- [ ] Implement model unloading for memory management
- [ ] Add health check endpoint for model status
- [ ] Handle CUDA out-of-memory errors gracefully

#### 1.2 OPUS Recording Format

**Goal**: Replace WAV with OPUS for efficient storage

**Rationale**:

- **Size Reduction**: 60-minute WAV (~170 MB) ‚Üí OPUS (~15-25 MB)
- **Quality**: Excellent voice quality at 32-64 kbps
- **Compatibility**: FFmpeg already available in project

**Implementation**:

```rust
// src-tauri/src/audio/recording.rs
pub struct OpusRecorder {
    encoder: ffmpeg::Encoder,
    output_path: PathBuf,
    sample_rate: u32,
    channels: u16,
}

impl OpusRecorder {
    pub fn new(output_path: PathBuf) -> Result<Self, String> {
        let encoder = ffmpeg::Encoder::new()
            .codec("libopus")
            .sample_rate(24000)
            .channels(1)
            .bitrate(48000) // 48 kbps
            .build()?;

        Ok(Self { encoder, output_path, sample_rate: 24000, channels: 1 })
    }

    pub fn write_samples(&mut self, samples: &[f32]) -> Result<(), String> {
        self.encoder.encode(samples)?;
        Ok(())
    }

    pub fn finalize(self) -> Result<(), String> {
        self.encoder.flush()?;
        self.encoder.finish()?;
        Ok(())
    }
}
```

**Sidecar Decoding**:

```python
# audio_processor.py
import ffmpeg

def decode_opus(opus_path: str) -> np.ndarray:
    """Decode OPUS to PCM for model input"""
    out, _ = (
        ffmpeg
        .input(opus_path)
        .output('pipe:', format='f32le', acodec='pcm_f32le', ar=24000, ac=1)
        .run(capture_stdout=True, capture_stderr=True)
    )
    return np.frombuffer(out, np.float32)
```

**Settings**:

- OPUS bitrate: 32 kbps / 48 kbps / 64 kbps (dropdown)
- Auto-save recordings: Yes / No (checkbox)
- Recording directory: Path selector

**Tasks**:

- [ ] Implement OPUS encoder in Rust (FFmpeg bindings)
- [ ] Add OPUS encoder to Output Capture pipeline
- [ ] Implement OPUS decoder in sidecar
- [ ] Add bitrate configuration to settings
- [ ] Test recording quality at different bitrates
- [ ] Add recording file size display in UI

#### 1.3 Parallel Analysis Mode

**Goal**: Optionally run Whisper + VibeVoice simultaneously

**Memory Considerations**:

| Mode            | Whisper | VibeVoice | Total    | Safe for 16GB?  |
|-----------------|---------|-----------|----------|-----------------|
| Sequential FP16 | 2-4 GB  | 14-16 GB  | N/A      | ‚úÖ Yes          |
| Sequential INT8 | 2-4 GB  | 7-8 GB    | N/A      | ‚úÖ Yes          |
| Parallel FP16   | 2-4 GB  | 14-16 GB  | 16-20 GB | ‚ö†Ô∏è Tight/Risky  |
| Parallel INT8   | 2-4 GB  | 7-8 GB    | 9-12 GB  | ‚úÖ Yes          |

**Recommendation**: Default to Sequential FP16

**Implementation**:

```rust
// src-tauri/src/transcription/parallel.rs
pub struct ParallelAnalyzer {
    whisper_handle: Option<JoinHandle<()>>,
    vibevoice_handle: Option<JoinHandle<()>>,
}

impl ParallelAnalyzer {
    pub fn analyze(&mut self, audio_path: PathBuf, settings: &Settings) -> Result<(), String> {
        if settings.parallel_analysis {
            // Start both processes simultaneously
            self.whisper_handle = Some(tokio::spawn(run_whisper(audio_path.clone())));
            self.vibevoice_handle = Some(tokio::spawn(run_vibevoice(audio_path)));
        } else {
            // Sequential: Whisper first, then VibeVoice
            run_whisper(audio_path.clone())?;
            run_vibevoice(audio_path)?;
        }
        Ok(())
    }
}
```

**Settings**:

- Parallel analysis: Enabled / Disabled (checkbox with warning)
- Warning text: "Requires INT8 mode or may exceed VRAM limit"

**Tasks**:

- [ ] Implement parallel analysis coordinator
- [ ] Add VRAM monitoring during parallel analysis
- [ ] Implement fallback to sequential on OOM error
- [ ] Add progress indicators for both processes
- [ ] Test memory usage in both modes

### 2. Auto-Processing Pipeline

#### 2.1 Auto-Analysis After Meeting

**Goal**: Automatically trigger VibeVoice-ASR analysis when recording ends

**Workflow**:

1. User stops recording (PTT release or Stop button)
2. Recording saved to OPUS file
3. If auto-analysis enabled:
   - Show notification: "Analyzing meeting..."
   - POST to sidecar /transcribe endpoint
   - Display progress in UI
4. Results shown in speaker-diarized transcript view

**Implementation**:

```rust
// src-tauri/src/lib.rs
#[tauri::command]
async fn on_recording_end(
    state: State<'_, AppState>,
    window: Window,
) -> Result<(), String> {
    let settings = state.settings.lock().unwrap().clone();
    let recording_path = state.last_recording_path.lock().unwrap().clone();

    if settings.auto_analysis && recording_path.is_some() {
        window.emit("analysis:start", serde_json::json!({
            "path": recording_path
        })).unwrap();

        // Trigger analysis in background
        tokio::spawn(async move {
            let result = analyze_recording(recording_path.unwrap()).await;
            window.emit("analysis:complete", result).unwrap();
        });
    }

    Ok(())
}
```

**Settings**:

- Auto-analyze recordings: Yes / No (checkbox)
- Analysis delay: 0s / 5s / 10s (dropdown) - delay before starting analysis

**Tasks**:

- [ ] Add auto-analysis toggle to settings
- [ ] Implement recording end event handler
- [ ] Add analysis progress UI
- [ ] Implement background analysis task
- [ ] Add cancellation support for running analysis

#### 2.2 Auto-Chapter Generation

**Goal**: Automatically detect and create chapter markers

**Detection Methods**:

1. **Speaker Changes**: New chapter when speaker switches after silence
2. **Long Silence**: Pauses > 10 seconds indicate topic shifts
3. **Semantic Analysis** (optional): Use LLM to detect topic changes

**Algorithm**:

```python
# chapter_generator.py
def generate_chapters(segments: List[Segment]) -> List[Chapter]:
    chapters = []
    current_chapter = None
    last_speaker = None
    silence_threshold = 10.0  # seconds

    for i, segment in enumerate(segments):
        # Check for silence gap
        if i > 0:
            gap = segment.start - segments[i-1].end
            if gap > silence_threshold:
                # Close current chapter
                if current_chapter:
                    chapters.append(current_chapter)
                # Start new chapter
                current_chapter = Chapter(
                    start=segment.start,
                    label=f"Chapter {len(chapters) + 1}"
                )

        # Check for speaker change after silence
        if segment.speaker_id != last_speaker and gap > 3.0:
            if current_chapter:
                chapters.append(current_chapter)
            current_chapter = Chapter(
                start=segment.start,
                label=f"Chapter {len(chapters) + 1}"
            )

        last_speaker = segment.speaker_id

    return chapters
```

**Data Structure**:

```typescript
interface Chapter {
  id: string;
  start_time: number; // seconds
  end_time: number;
  label: string; // "Chapter 1", "Introduction", etc.
  speaker_count: number;
  segment_ids: string[];
}
```

**Settings**:

- Auto-generate chapters: Yes / No (checkbox)
- Silence threshold: 5s / 10s / 15s (slider)
- Minimum chapter length: 1 min / 2 min / 5 min (dropdown)

**Tasks**:

- [ ] Implement chapter detection algorithm
- [ ] Add chapter data structure to transcript
- [ ] Implement chapter navigation UI
- [ ] Add chapter label editing
- [ ] Add chapter merging/splitting tools

#### 2.3 Meeting Minutes Generation

**Goal**: AI-powered extraction of key information from meetings

**Output Format**:

```markdown
# Meeting Minutes
**Date**: 2026-02-13
**Duration**: 45 minutes
**Participants**: Speaker 1, Speaker 2, Speaker 3

## Summary
Brief overview of the meeting's purpose and outcomes.

## Action Items
- [ ] Task 1 (Assigned to: Speaker 1, Due: 2026-02-20)
- [ ] Task 2 (Assigned to: Speaker 2, Due: 2026-02-25)

## Decisions Made
1. Decision about project direction
2. Budget approval for Q1

## Key Discussion Points
- Topic 1: Discussion summary
- Topic 2: Discussion summary

## Next Steps
- Schedule follow-up meeting
- Review draft proposal
```

**Implementation**:

```python
# minutes_generator.py
from anthropic import Anthropic

def generate_minutes(segments: List[Segment]) -> str:
    """Generate meeting minutes using Claude API"""
    client = Anthropic(api_key=get_api_key())

    # Build transcript with speaker attribution
    transcript = format_transcript_for_llm(segments)

    prompt = f"""
    Analyze this meeting transcript and generate structured meeting minutes.

    Include:
    1. Brief summary (2-3 sentences)
    2. Action items with assigned speakers
    3. Key decisions made
    4. Important discussion points
    5. Next steps

    Transcript:
    {transcript}

    Format the output as markdown.
    """

    response = client.messages.create(
        model="claude-sonnet-4.5",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )

    return response.content[0].text
```

**Settings**:

- Auto-generate minutes: Yes / No (checkbox)
- AI provider: Claude / OpenAI / Local (dropdown)
- Minutes template: Standard / Detailed / Brief (dropdown)

**Tasks**:

- [ ] Implement minutes generation with Claude API
- [ ] Add support for OpenAI as alternative provider
- [ ] Create customizable templates
- [ ] Add manual editing UI for generated minutes
- [ ] Implement PDF export via Pandoc

#### 2.4 Meeting Summary/Analysis

**Goal**: High-level meeting insights and statistics

**Output**:

```json
{
  "duration_seconds": 2700,
  "speaker_count": 3,
  "speakers": [
    {
      "id": "speaker_1",
      "speaking_time": 1200,
      "percentage": 44.4,
      "turn_count": 45
    }
  ],
  "topics": [
    {
      "label": "Budget Discussion",
      "start": 300,
      "end": 900,
      "speakers": ["speaker_1", "speaker_2"]
    }
  ],
  "sentiment": "positive",
  "key_moments": [
    {
      "time": 450,
      "description": "Decision made on project scope",
      "speaker": "speaker_1"
    }
  ]
}
```

**Implementation**:

```rust
// src-tauri/src/analysis/summary.rs
pub struct MeetingSummary {
    pub duration_seconds: u64,
    pub speaker_stats: Vec<SpeakerStats>,
    pub topics: Vec<Topic>,
    pub key_moments: Vec<KeyMoment>,
}

impl MeetingSummary {
    pub fn from_segments(segments: &[Segment]) -> Self {
        let speaker_stats = calculate_speaker_stats(segments);
        let topics = extract_topics(segments);
        let key_moments = identify_key_moments(segments);

        Self {
            duration_seconds: calculate_duration(segments),
            speaker_stats,
            topics,
            key_moments,
        }
    }
}
```

**UI Display**:

- Speaker participation pie chart
- Timeline view with topic markers
- Key moments list with jump-to functionality
- Export to JSON/CSV

**Tasks**:

- [ ] Implement speaker statistics calculation
- [ ] Add topic extraction (keywords + LLM)
- [ ] Create summary visualization components
- [ ] Add export functionality
- [ ] Implement comparison view for multiple meetings

### 3. Output Formats

#### 3.1 Speaker-Diarized Transcript

**Visual Format** (HTML):

```html
<div class="transcript">
  <div class="segment speaker-1">
    <span class="speaker-label">Speaker 1</span>
    <span class="timestamp">00:00:15</span>
    <p class="text">Let's start the meeting by discussing the budget.</p>
  </div>
  <div class="segment speaker-2">
    <span class="speaker-label">Speaker 2</span>
    <span class="timestamp">00:00:22</span>
    <p class="text">I've prepared the Q1 projections.</p>
  </div>
</div>

<style>
.speaker-1 { border-left: 4px solid #4CAF50; }
.speaker-2 { border-left: 4px solid #2196F3; }
.speaker-3 { border-left: 4px solid #FF9800; }
</style>
```

**JSON Format**:

```json
{
  "recording_date": "2026-02-13T14:30:00Z",
  "duration": 2700,
  "speakers": ["speaker_1", "speaker_2", "speaker_3"],
  "segments": [
    {
      "id": "seg_001",
      "speaker_id": "speaker_1",
      "start": 15.2,
      "end": 22.5,
      "text": "Let's start the meeting by discussing the budget.",
      "confidence": 0.95
    }
  ]
}
```

**Tasks**:

- [ ] Design transcript UI with color-coded speakers
- [ ] Implement speaker label editing
- [ ] Add search/filter by speaker
- [ ] Implement timeline scrubbing with audio sync
- [ ] Add export to HTML/JSON

#### 3.2 Video with Karaoke-Style Overlay

**Goal**: Generate video with text overlay showing current speaker and text

**Implementation** (FFmpeg):

```bash
ffmpeg -i meeting.opus -i transcript.ass \
  -c:a copy -c:v libx264 \
  -vf "subtitles=transcript.ass" \
  meeting_with_subs.mp4
```

**ASS Subtitle Format**:

```
[Events]
Dialogue: 0,0:00:15.20,0:00:22.50,Speaker1,,0,0,0,,Let's start the meeting
Dialogue: 0,0:00:22.50,0:00:28.10,Speaker2,,0,0,0,,I've prepared the Q1 projections
```

**Styling**:

- Speaker-specific colors
- Karaoke word-by-word highlighting
- Speaker name badge
- Progress bar

**Tasks**:

- [ ] Generate ASS subtitles from segments
- [ ] Implement word-level timing (via Whisper alignment)
- [ ] Create FFmpeg video export pipeline
- [ ] Add style customization options
- [ ] Implement progress indicator for export

#### 3.3 Meeting Minutes Export (MD/PDF)

**Markdown Export**:

Direct markdown output from minutes generator (see 2.3)

**PDF Export** (via Pandoc):

```rust
// src-tauri/src/export/pdf.rs
pub fn export_to_pdf(markdown: &str, output_path: &Path) -> Result<(), String> {
    Command::new("pandoc")
        .arg("-f").arg("markdown")
        .arg("-t").arg("pdf")
        .arg("-o").arg(output_path)
        .arg("--pdf-engine=xelatex")
        .arg("--template=meeting-template.latex")
        .stdin(Stdio::piped())
        .spawn()
        .and_then(|mut child| {
            child.stdin.as_mut().unwrap().write_all(markdown.as_bytes())?;
            child.wait()
        })
        .map_err(|e| format!("PDF export failed: {}", e))?;

    Ok(())
}
```

**Tasks**:

- [ ] Bundle Pandoc with installer
- [ ] Create professional PDF template
- [ ] Add customization options (logo, colors)
- [ ] Implement batch export for multiple meetings
- [ ] Add email integration for sending minutes

#### 3.4 Chapter-Indexed Transcript

**Format** (JSON):

```json
{
  "chapters": [
    {
      "id": "ch_001",
      "label": "Budget Discussion",
      "start": 0,
      "end": 600,
      "segments": ["seg_001", "seg_002", "seg_003"],
      "speakers": ["speaker_1", "speaker_2"],
      "summary": "Team discussed Q1 budget allocations"
    }
  ]
}
```

**UI**:

- Chapter navigation sidebar
- Jump to chapter functionality
- Chapter summary tooltips
- Export with chapters as markdown sections

**Tasks**:

- [ ] Implement chapter-indexed JSON export
- [ ] Create chapter navigation UI component
- [ ] Add chapter summary generation
- [ ] Implement markdown export with chapter headers

### 4. Settings UI

#### 4.1 Quality Presets

**UI Component**:

```typescript
interface QualitySettings {
  precision: 'fp16' | 'int8';
  parallelAnalysis: boolean;
}

// Settings panel
<div class="quality-settings">
  <label>Model Precision</label>
  <select value={precision} onChange={setPrecision}>
    <option value="fp16">FP16 (Highest Quality, 14-16GB VRAM)</option>
    <option value="int8">INT8 (Good Quality, 7-8GB VRAM)</option>
  </select>

  <label>
    <input type="checkbox" checked={parallelAnalysis} onChange={setParallel} />
    Enable Parallel Analysis
  </label>
  {parallelAnalysis && precision === 'fp16' && (
    <div class="warning">
      ‚ö†Ô∏è Parallel FP16 mode may exceed VRAM limit. Consider INT8.
    </div>
  )}
</div>
```

**Tasks**:

- [ ] Design settings panel UI
- [ ] Implement precision selector
- [ ] Add parallel analysis toggle with warnings
- [ ] Add VRAM usage indicator (live monitoring)
- [ ] Persist settings across sessions

#### 4.2 Auto-Processing Toggles

**UI Component**:

```typescript
interface AutoProcessingSettings {
  autoAnalysis: boolean;
  autoChapters: boolean;
  autoMinutes: boolean;
  analysisDelay: number; // seconds
}

// Settings panel
<div class="auto-processing-settings">
  <h3>Auto-Processing</h3>

  <label>
    <input type="checkbox" checked={autoAnalysis} />
    Auto-analyze after recording
  </label>

  {autoAnalysis && (
    <div class="sub-settings">
      <label>
        <input type="checkbox" checked={autoChapters} />
        Auto-generate chapters
      </label>

      <label>
        <input type="checkbox" checked={autoMinutes} />
        Auto-generate meeting minutes
      </label>

      <label>
        Analysis delay: {analysisDelay}s
        <input type="range" min="0" max="30" value={analysisDelay} />
      </label>
    </div>
  )}
</div>
```

**Tasks**:

- [ ] Design auto-processing settings UI
- [ ] Implement toggle controls
- [ ] Add dependency handling (minutes requires analysis)
- [ ] Add preview/test functionality
- [ ] Implement settings validation

### 5. Model Update Strategy

#### 5.1 Version Monitoring

**Implementation**:

```rust
// src-tauri/src/models/update_monitor.rs
pub struct ModelUpdateMonitor {
    current_version: String,
    check_interval: Duration,
}

impl ModelUpdateMonitor {
    pub async fn check_for_updates(&self) -> Result<Option<ModelUpdate>, String> {
        // Check HuggingFace API
        let response = reqwest::get(
            "https://huggingface.co/api/models/microsoft/vibevoice-asr"
        ).await?;

        let latest = parse_latest_version(&response)?;

        if latest.version > self.current_version {
            let release_notes = fetch_release_notes(&latest).await?;
            Ok(Some(ModelUpdate {
                version: latest.version,
                benefits: parse_benefits(&release_notes),
                download_url: latest.download_url,
                checksum: latest.checksum,
            }))
        } else {
            Ok(None)
        }
    }
}
```

**Schedule**:

- Check on app startup (once per day max)
- Manual check button in settings
- Background check every 7 days

**Tasks**:

- [ ] Implement HuggingFace API integration
- [ ] Add version comparison logic
- [ ] Implement release notes fetching
- [ ] Add manual check button to UI
- [ ] Implement update check scheduling

#### 5.2 User Notification

**UI Component**:

```typescript
interface UpdateNotification {
  version: string;
  benefits: string[];
  downloadSize: number;
}

// Notification modal
<div class="update-notification">
  <h2>üéâ New VibeVoice-ASR Version Available</h2>
  <p><strong>Version {update.version}</strong></p>

  <h3>What's New:</h3>
  <ul>
    {update.benefits.map(benefit => (
      <li>{benefit}</li>
    ))}
  </ul>

  <p>Download size: {formatSize(update.downloadSize)}</p>

  <div class="actions">
    <button onClick={updateNow}>Update Now</button>
    <button onClick={remindLater}>Remind Me Later</button>
    <button onClick={skipVersion}>Skip This Version</button>
  </div>
</div>
```

**Tasks**:

- [ ] Design update notification UI
- [ ] Implement benefit parsing from release notes
- [ ] Add download progress indicator
- [ ] Implement "Skip This Version" persistence
- [ ] Add automatic rollback on update failure

#### 5.3 Update Flow

**Process**:

1. Download new model to temporary directory
2. Verify checksum
3. Stop sidecar (if running)
4. Backup current model
5. Replace model files
6. Restart sidecar
7. Test new model (health check)
8. On failure: Rollback to backup

**Implementation**:

```rust
// src-tauri/src/models/updater.rs
pub async fn update_model(update: ModelUpdate) -> Result<(), String> {
    let temp_path = download_model(&update.download_url).await?;
    verify_checksum(&temp_path, &update.checksum)?;

    stop_sidecar().await?;

    let backup_path = backup_current_model()?;

    match replace_model(&temp_path) {
        Ok(_) => {
            start_sidecar().await?;
            if !health_check().await? {
                rollback_model(&backup_path)?;
                return Err("Update failed health check, rolled back".into());
            }
            Ok(())
        }
        Err(e) => {
            rollback_model(&backup_path)?;
            Err(e)
        }
    }
}
```

**Tasks**:

- [ ] Implement model download with progress
- [ ] Add checksum verification
- [ ] Implement backup/rollback mechanism
- [ ] Add health check after update
- [ ] Implement update failure notifications

## Implementation Phases

### Phase 1: Core VibeVoice Integration (Weeks 1-2)

**Week 1: Sidecar Foundation**

- [ ] Set up sidecar project structure
- [ ] Implement FastAPI server with /transcribe endpoint
- [ ] Add FP16 model loading
- [ ] Implement basic speaker diarization
- [ ] Add health check endpoint
- [ ] Test with sample audio files

**Week 2: Rust Integration**

- [ ] Implement sidecar process management in Rust
- [ ] Add HTTP client for sidecar communication
- [ ] Create speaker-diarized transcript data structures
- [ ] Implement basic UI for analysis results
- [ ] Add error handling and retry logic

### Phase 2: OPUS Recording (Week 3)

- [ ] Implement FFmpeg-based OPUS encoder
- [ ] Integrate OPUS encoder into Output Capture
- [ ] Add OPUS decoder to sidecar
- [ ] Implement bitrate configuration
- [ ] Test recording quality and file sizes
- [ ] Add migration path for existing WAV recordings

### Phase 3: Auto-Processing Pipeline (Weeks 4-5)

**Week 4: Chapters & Analysis**

- [ ] Implement auto-analysis trigger
- [ ] Add chapter detection algorithm
- [ ] Create chapter navigation UI
- [ ] Implement meeting summary generation
- [ ] Add speaker statistics calculation

**Week 5: Minutes Generation**

- [ ] Integrate Claude API for minutes
- [ ] Implement minutes template system
- [ ] Add manual editing UI for minutes
- [ ] Implement MD/PDF export
- [ ] Test with real meeting recordings

### Phase 4: Settings & Model Updates (Week 6)

- [ ] Design and implement settings UI
- [ ] Add precision selector (FP16/INT8)
- [ ] Add auto-processing toggles
- [ ] Implement model update monitor
- [ ] Create update notification UI
- [ ] Add VRAM usage monitoring

### Phase 5: Export Formats & Polish (Weeks 7-8)

**Week 7: Export Formats**

- [ ] Implement video export with karaoke overlay
- [ ] Add chapter-indexed transcript export
- [ ] Implement PDF export with Pandoc
- [ ] Add batch export functionality

**Week 8: Testing & Documentation**

- [ ] Integration testing (end-to-end flows)
- [ ] Performance testing (VRAM, CPU, disk usage)
- [ ] User acceptance testing
- [ ] Documentation (user guide, API docs)
- [ ] Bug fixes and polish

## Technical Considerations

### Performance

**Memory Management**:

- Monitor VRAM usage during analysis
- Implement graceful degradation on OOM
- Add swap-to-disk option for large recordings (>60 min)

**Processing Speed**:

- Target: Real-time or faster (1 hour audio in <60 minutes)
- Optimization: Batch processing for multiple recordings
- Caching: Store intermediate results for re-analysis

**Storage**:

- OPUS recordings: ~15-25 MB per 60 minutes
- Transcript JSON: ~500 KB per 60 minutes
- Total per meeting: ~30 MB (including video export)

### Security

**API Keys**:

- Encrypt Claude API keys in settings
- Use OS keychain for storage (Windows Credential Manager)
- Validate keys before use

**Sidecar Communication**:

- Use localhost-only HTTP (no external access)
- Add authentication token for requests
- Validate all input paths (no directory traversal)

### UX Considerations

**Progress Feedback**:

- Show analysis progress percentage
- Display current processing step
- Allow cancellation at any time

**Error Handling**:

- Clear error messages for common issues
- Suggest solutions (e.g., "Out of VRAM? Try INT8 mode")
- Log detailed errors for debugging

**First-Time Setup**:

- Model download wizard
- Hardware detection (VRAM, CUDA)
- Settings recommendations based on hardware

## Testing Strategy

### Unit Tests

- [ ] Model loader (FP16/INT8)
- [ ] OPUS encoder/decoder
- [ ] Chapter detection algorithm
- [ ] Speaker statistics calculation
- [ ] Export formatters

### Integration Tests

- [ ] Full analysis pipeline (recording ‚Üí analysis ‚Üí export)
- [ ] Sidecar lifecycle (start/stop/health)
- [ ] Auto-processing triggers
- [ ] Model updates

### Performance Tests

- [ ] VRAM usage (FP16 vs INT8, parallel vs sequential)
- [ ] Processing speed (various audio lengths)
- [ ] File size (OPUS vs WAV)
- [ ] Export speed (video, PDF)

### User Acceptance Tests

- [ ] Record and analyze a real meeting
- [ ] Verify speaker diarization accuracy
- [ ] Test chapter generation quality
- [ ] Review generated meeting minutes
- [ ] Test all export formats

## Documentation

### User Documentation

- [ ] Getting Started guide
- [ ] Hardware requirements
- [ ] Settings reference
- [ ] Export format examples
- [ ] Troubleshooting guide

### Developer Documentation

- [ ] Architecture overview
- [ ] Sidecar API reference
- [ ] Adding new export formats
- [ ] Model update protocol
- [ ] Testing guide

## Dependencies

### New Dependencies

**Rust**:

- `ffmpeg-next`: FFmpeg bindings for OPUS encoding
- `tokio`: Async runtime for parallel processing
- `reqwest`: HTTP client for sidecar communication

**Python** (Sidecar):

- `fastapi`: Web framework
- `uvicorn`: ASGI server
- `transformers`: Model loading
- `torch`: PyTorch for inference
- `ffmpeg-python`: Audio processing

**Bundled Tools**:

- Pandoc: PDF export
- FFmpeg: Video export with subtitles

## Risks & Mitigations

### Risk 1: VRAM Limitations

**Risk**: 16GB VRAM insufficient for FP16 + Whisper parallel mode

**Mitigation**:

- Default to sequential processing
- Offer INT8 mode for parallel processing
- Add VRAM monitoring and warnings
- Implement swap-to-disk fallback

### Risk 2: Model Quality

**Risk**: INT8 quantization degrades speaker diarization accuracy

**Mitigation**:

- Test INT8 vs FP16 accuracy before release
- Document quality differences in settings
- Allow users to choose based on hardware
- Consider 4-bit quantization if 8-bit insufficient

### Risk 3: Processing Speed

**Risk**: Analysis too slow for long meetings (>60 minutes)

**Mitigation**:

- Optimize model loading (lazy, caching)
- Use Flash Attention 2 for speed
- Implement chunked processing for >60 min
- Add batch processing for multiple meetings

### Risk 4: API Costs

**Risk**: Claude API costs for meeting minutes generation

**Mitigation**:

- Make minutes generation optional
- Support OpenAI as cheaper alternative
- Add local LLM option (Ollama integration)
- Implement token usage tracking

## Success Metrics

**Functional**:

- [ ] Speaker diarization accuracy >90%
- [ ] Chapter detection recall >80%
- [ ] Meeting minutes quality rated 4+/5 by users
- [ ] All export formats working correctly

**Performance**:

- [ ] Analysis speed: 1x real-time or faster
- [ ] VRAM usage within limits (FP16: <16GB, INT8: <8GB)
- [ ] OPUS file size <20% of WAV equivalent
- [ ] UI remains responsive during analysis

**Usability**:

- [ ] First-time setup <5 minutes
- [ ] Auto-processing requires zero manual intervention
- [ ] Export formats intuitive and high-quality
- [ ] Model updates smooth and reliable

## Future Enhancements (v0.7.0+)

- Real-time speaker diarization (parallel mode optimization)
- Multi-meeting comparison and analytics
- Team meeting insights dashboard
- Integration with calendar apps (auto-record scheduled meetings)
- Cloud storage integration (OneDrive, Google Drive)
- Collaboration features (shared meeting transcripts)
- Advanced video editing (clip extraction, highlights)
